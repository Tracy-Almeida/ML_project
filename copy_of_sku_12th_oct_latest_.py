# -*- coding: utf-8 -*-
"""Copy of sku 12th oct latest .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/142U9JOEu8GWljdXEN0FEWzQLNydFiHAU
"""

# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error



# Load the dataset into a DataFrame (use the appropriate filename from the uploaded files)
data = pd.read_csv('/content/SalesKaggle3.csv')

# Display the first few rows of the data
data.head()

# Step 2: Handling missing values
data['SoldFlag'].fillna(0, inplace=True)
data['SoldCount'].fillna(0, inplace=True)

# Verify there are no more missing values
data.isnull().sum()

#checking outliers
import seaborn as sns
import matplotlib.pyplot as plt

# Set up the matplotlib figure
plt.figure(figsize=(16, 8))

# Generate a boxplot for all numerical columns in the dataframe
sns.boxplot(data=data) # Changed df to data

# Set title
plt.title('Boxplot for All Numerical Features')

# Show plot
plt.xticks(rotation=45)  # Rotate the x-axis labels if needed
plt.tight_layout()
plt.show()

"""SKU_number: The maximum value (3,960,788) is significantly higher than the upper quartile (904,751), indicating potential outliers.

StrengthFactor: With a maximum of 17.3 million, it's an extreme outlier, far higher than the third quartile (1.43 million). This column may need scaling or outlier treatment.

PriceReg: Prices range from 0 to 12,671, with a median of only 69.95. This large variation suggests potential outliers.

ItemCount: The range is from 0 to 2,542, with most values concentrated below 50. Outliers are likely in this column.

LowUserPrice and LowNetPrice: With maximum values of 14,140 and 19,138 respectively, these prices have extreme outliers compared to the quartiles.

Outlier treatment on these columns (e.g., capping, transformation, or removal) could improve your regression model's performance for stock prediction. ​
"""

# Step 3: Handling outliers using IQR method
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return series.clip(lower=lower_bound, upper=upper_bound)

# Applying outlier capping to relevant columns
columns_with_outliers = ['SKU_number', 'StrengthFactor', 'PriceReg', 'ItemCount', 'LowUserPrice', 'LowNetPrice']
for column in columns_with_outliers:
    data[column] = cap_outliers(data[column])

# Verify the changes
data[columns_with_outliers].describe()

import seaborn as sns

# Select numerical columns
numerical_columns = data.select_dtypes(include=np.number).columns

# Visualizing outliers for all numerical columns using boxplots
plt.figure(figsize=(15, 10))
sns.boxplot(data=data[numerical_columns])
plt.xticks(rotation=90)  # Rotate x-axis labels if needed
plt.title("Boxplot for All Numerical Features")
plt.show()

"""EDA"""

#Total SoldCount by SKU_number
sku_sales = data.groupby('SKU_number')['SoldCount'].sum().sort_values(ascending=False)
sku_sales.head(10).plot(kind='bar', figsize=(10, 6), color='skyblue')
plt.title('Top 10 SKU by SoldCount')
plt.xlabel('SKU_number')
plt.ylabel('Total SoldCount')
plt.show()

#Marketing type vs sold count
marketing_sales = data.groupby('MarketingType')['SoldCount'].mean().sort_values()
marketing_sales.plot(kind='bar', figsize=(8, 5), color='teal')
plt.title('Average SoldCount by MarketingType')
plt.xlabel('MarketingType')
plt.ylabel('Average SoldCount')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.scatterplot(x=data['ItemCount'], y=data['SoldCount'], alpha=0.7, color='blue')
plt.title('SoldCount vs ItemCount')
plt.xlabel('ItemCount')
plt.ylabel('SoldCount')
plt.grid(True)
plt.show()

#  One-Hot Encoding to the categorical columns

categorical_columns = ['File_Type', 'MarketingType']
df_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

df_encoded.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Extracting  the target variable
y = df_encoded['SoldCount']

# 1.  Histogram
plt.figure(figsize=(12, 6))
plt.hist(y, bins=30, edgecolor='k', alpha=0.7)
plt.title('Distribution of SoldCount')
plt.xlabel('SoldCount')
plt.ylabel('Frequency')
plt.grid()
plt.show()

# 2.  Boxplot
plt.figure(figsize=(12, 6))
sns.boxplot(y)
plt.title('Boxplot of SoldCount')
plt.ylabel('SoldCount')
plt.grid()
plt.show()

# 3. Statistical Summary
summary_stats = y.describe()
print("Statistical Summary of SoldCount:")
print(summary_stats)

# 4. Count Unique Values
unique_counts = y.nunique()
print(f"Number of unique SoldCount values: {unique_counts}")

# Statistical Summary
summary_stats = df_encoded['SoldCount'].describe()
print("Statistical Summary of SoldCount:")
print(summary_stats)

df_encoded['SoldCount_log'] = np.log1p(df_encoded['SoldCount'])

"""OUR TARGET VARIABLE WAS POSITIVELY SKEWED SO APPLIED LOG TRANSFORMATION"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
numeric_cols = df_encoded.select_dtypes(include=np.number).columns #defined the numeric columns

#scaling
scaler = StandardScaler()
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])



from sklearn.model_selection import train_test_split

# Define the target variable (SoldCount) and the features
X = data.drop(columns=['SoldCount'])  # Features
# Use the log-transformed SoldCount (SoldCount_log)
y = df_encoded['SoldCount_log']

# Split the dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shape of the datasets
print(X_train.shape, X_test.shape)

"""after applying oversampling on linear regression"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


# Step 2: Handle non-numeric columns (if any)
non_numeric_cols = df_encoded.select_dtypes(include=['object']).columns
df_encoded = pd.get_dummies(df_encoded, columns=non_numeric_cols, drop_first=True)

# Step 3: Analyze the target variable and determine the threshold for oversampling
y = df_encoded['SoldCount']  # Assuming 'SoldCount' is your target variable
threshold = y.quantile(0.95)  # Define threshold for the minority class
high_soldcount = df_encoded[df_encoded['SoldCount'] > threshold]

# Step 4: Manually oversample the rows with high SoldCount
df_oversampled = pd.concat([df_encoded, high_soldcount.sample(n=1000, replace=True, random_state=42)])

# Step 5: Define features (X) and target (y) from the oversampled data
X = df_oversampled.drop(columns=['SoldCount'])
y = df_oversampled['SoldCount']

# Step 6: Split the oversampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Train your model on the oversampled data
model = LinearRegression()
model.fit(X_train, y_train)

# Step 8: Predict and evaluate the model
y_pred = model.predict(X_test)

# Evaluate performance
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R² Score: {r2}")

"""to see how the data is balanced after oversapling"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot histogram of the oversampled target variable
plt.figure(figsize=(10, 6))
sns.histplot(df_oversampled['SoldCount'], bins=50, kde=True)
plt.title('Distribution of SoldCount After Oversampling')
plt.xlabel('SoldCount')
plt.ylabel('Frequency')
plt.show()

"""Random forest applying oversamling to balance data"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Define features (X) and target (y)
X = df_encoded.drop(columns=['SoldCount'])  # Features
y = df_encoded['SoldCount']  # Target

# Step 2: Analyze the target variable and determine the threshold for oversampling
threshold = y.quantile(0.90)  # Defined threshold for the minority class
high_soldcount = df_encoded[df_encoded['SoldCount'] > threshold]

# Step 3: Manually oversample the rows with high SoldCount
df_oversampled = pd.concat([df_encoded, high_soldcount.sample(n=1000, replace=True, random_state=42)])

# Step 4: Define features (X) and target (y) from the oversampled data
X_oversampled = df_oversampled.drop(columns=['SoldCount'])
y_oversampled = df_oversampled['SoldCount']

# Step 5: Split the oversampled dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.2, random_state=42)

# Step 6: Initialize the Random Forest Regressor using default parameters
rf_reg = RandomForestRegressor(random_state=42)

# Step 7: Fit the model to the training data
rf_reg.fit(X_train, y_train)

# Step 8: Make predictions on the testing data
y_pred_rf = rf_reg.predict(X_test)

# Step 9: Evaluate the model using various metrics
rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)  # Root Mean Squared Error
mae_rf = mean_absolute_error(y_test, y_pred_rf)  # Mean Absolute Error
r2_rf = r2_score(y_test, y_pred_rf)  # R² Score

# Print out the evaluation results
print(f"Random Forest RMSE: {rmse_rf}")
print(f"Random Forest MAE: {mae_rf}")
print(f"Random Forest R² Score: {r2_rf}")


# Optional: Display feature importances
importances = rf_reg.feature_importances_
feature_importances = pd.DataFrame(importances, index=X.columns, columns=["Importance"]).sort_values("Importance", ascending=False)
print(feature_importances)

import matplotlib.pyplot as plt

# Scatter plot of predicted vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Line for perfect predictions
plt.xlabel('Actual SoldCount')
plt.ylabel('Predicted SoldCount')
plt.title('Actual vs. Predicted SoldCount')
plt.show()

"""to check distribution after appying oversampling to random forest"""

import matplotlib.pyplot as plt
import seaborn as sns


# Plot histogram of the oversampled target variable
plt.figure(figsize=(10, 6))
sns.histplot(df_oversampled['SoldCount'], bins=50, kde=True)
plt.title('Distribution of SoldCount After Oversampling')
plt.xlabel('SoldCount')
plt.ylabel('Frequency')
plt.axvline(x=y.mean(), color='r', linestyle='--', label='Mean SoldCount')
plt.axvline(x=y.quantile(0.90), color='g', linestyle='--', label='90th Percentile')  # 90th percentile line
plt.legend()
plt.show()



"""TO DEFINE A THRESHOLD VALUE AND CHECK THE HISTORICAL AND PREDICTED STOCKOUT AND OVERSTOCK

"""

#thresholds for stockout and overstock
# Use the same threshold for both historical and predicted data
threshold = 10
#  historical data for stockouts and overstocks
historical_data = df_encoded[['SoldCount', 'SoldFlag']]  # Use necessary columns from your DataFrame

# Identify stockout and overstock occurrences
historical_data['Stockout'] = historical_data['SoldCount'] <= threshold
historical_data['Overstock'] = historical_data['SoldCount'] > threshold

# Count occurrences
stockout_count = historical_data['Stockout'].sum()
overstock_count = historical_data['Overstock'].sum()

print(f"Historical Stockouts (threshold = {threshold}): {stockout_count}")
print(f"Historical Overstocks (threshold = {threshold}): {overstock_count}")

# stockout and overstock using the trained RandomForestRegressor model as randomforest gave us best accuracy
y_pred = rf_reg.predict(X_test)

# Use the same threshold for predicted data
predicted_stockouts = np.where(y_pred <= threshold)[0]
predicted_overstocks = np.where(y_pred > threshold)[0]

num_predicted_stockouts = len(predicted_stockouts)
num_predicted_overstocks = len(predicted_overstocks)

print(f"Number of predicted stockouts (threshold = {threshold}): {num_predicted_stockouts}")
print(f"Number of predicted overstocks (threshold = {threshold}): {num_predicted_overstocks}")#

"""FOUND THAT RANDOMFOREST WAS A GOOD MODEL FOR OUR PROJECT WITH HIGHEST ACCURACY"""