# -*- coding: utf-8 -*-
"""ML_CIA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174hf-kfKVbX1gPJd5I_6M6JRK1zNQEh2

# **Project submission by:**


---


**Ananya Dileep, 2309029, 21 <br>
Amruta Nair, 2309036, 27 <br>
Tracy Almeida, 2309037, 28**


---

# **Importing necessary libraries:**
---
"""

import numpy as np                              #for efficient mathematical computation
import seaborn as sns                       #for providing more efficient func’s for data visualization
import pandas as pd                             #for data manipulation and analysis
import matplotlib.pyplot as plt                 #for plotting graphs
import scipy as sp                              #for providing tools for statistical tasks

"""# **About the dataset:**


---


This dataset contains the basic bank details of customers in a company and has a lot of their credit-related information. The features included are:

**ID:** Represents a unique identification of an
entry  
**Customer ID:** Represents a unique identification of a person  
**Month:** Represents the month of the year  
**Name:** Represents the name of a person  
**Age:** Represents the age of the person  
**SSN:** Represents the social security number of a person  
**Occupation:** Represents the occupation of the person  
**Annual_Income:** Represents the annual income of the person  
**Monthly_Inhand_Salary:** Represents the monthly base salary of a person  
**Num_Bank_Accounts:** Represents the number of bank accounts a person holds  
**Num_Credit_Card:** Represents the number of other credit cards held by a person  
**Interest_Rate:** Represents the interest rate on credit card  
**Num_of_Loan:** Represents the number of loans taken from the bank  
**Delay_from_due_date:** Represents the average number of days delayed from the payment date  
**Num_of_Delayed_Payment:** Represents the average number of payments delayed by a person  
**Changed_Credit_Limit:** Represents the percentage change in credit card limit  
**Num_Credit_Inqueries:** Represents the number of credit card inquiries  
**Credit_Mix:** Represents the classification of the mix of credits  
**Outstanding_Debt:** Represents the remaining debt to be paid (in USD)  
**Credit_Utilization_Ratio:** Represents the utilization ratio of credit card  
**Credit_History_Age:** Represents the age of credit history of the person  
**Payment_of_Min_Amount:** Represents whether only the minimum amount was paid by the person  
**Total_EMI_per_Month:** Represents the monthly EMI payments (in USD)  
**Amount_invested_monthly:** Represents the monthly amount invested by the customer (in USD)  
**Payment_Behavior:** Represents the payment behavior of the customer (in USD)  
**Monthly_Balance:** Represents the monthly balance amount of the customer (in USD)

---
# **Goal:**
Given a person’s credit-related information of each month, build a machine learning model that can classify the credit score.
---


---

# **Loading the dataset**
The dataset is stored in a csv file format named “Credit_Score”.

---
"""

df = pd.read_csv("Credit_Score.csv")
df

"""# **Understanding data:**
Here, we are trying to gain some information about the dataset.


---


"""

df.shape                                  #returns the count of row and columns in the dataset

"""**Our dataset contains, 1 lakh(100000) rows and 27 columns (Including the target variable).**

---


"""

df.info()                          #returns the data type, count of each feature and mentioning if it contains null value or not

"""---


# **Preprocessing tasks**


---

## **Checking for null values :**
Here, we are checking if there are any missing/null values present in each feature and along with their count, to understand how much discrepancy is there in our dataset.




---
"""

df.isna().sum()

"""**Here we have observed that, the percentage of missing values in our dataset is less than 15%.**

---


"""

plt.figure(figsize=(12,8))
sns.heatmap(df.isnull())

"""**Here, we are trying to see the visual representation of the missing/null values present in our dataset. The white dashes in the figure, are the values which are missing in that specific feature.**"""

df['Num_of_Delayed_Payment']                #checking one of the columns having missing values

"""# **Checking for inconsistencies in data :**
1.   Through df.info we observed that, there were some numeric features in our dataset, whose data is not stored in numeric format like Annual Income, etc. So, we first identified those features and converted them to float.

2. Some of the values in some features had values in invalid format,such as "3251_" or negative values in columns, where it shouldn't be there like "Age" column. So we created a function to remove such inconsistencies.
---

**Viewing all the columns that are in 'object' format and viewing their unique values.**
"""

for i in df.columns:                          #checking all columns in the object format
    if df[i].dtype == type(object):
        print(i,end=': ')
        print('\n',df[i].unique())
        print()

def replace_(value:str):                 #created a func to extract meaningful values by removing the negative sign and underscores
  if '_' in str(value):
    return value.split('_')[0]
  elif '-' in str(value):
    return value.split('-')[1]
  else:
    return value

"""**Applying the above function replace_, to the respective
columns below and converting their datatype to numeric i.e. (float or int)**
"""

df['Num_of_Delayed_Payment']=df['Num_of_Delayed_Payment'].apply(replace_)
df['Num_of_Delayed_Payment']=df['Num_of_Delayed_Payment'].astype('float64')

df['Num_of_Delayed_Payment'].value_counts()      #to check if the discrepencies are removed after applying func

df['Annual_Income']=df['Annual_Income'].apply(replace_)
df['Annual_Income']=df['Annual_Income'].astype("float")

df['Age']=df['Age'].apply(replace_)
df['Age']=df['Age'].astype('int64')

df['Num_of_Loan']=df['Num_of_Loan'].apply(replace_)
df['Num_of_Loan']=df['Num_of_Loan'].astype('int64')

df['Outstanding_Debt']=df['Outstanding_Debt'].apply(replace_)
df['Outstanding_Debt']=df['Outstanding_Debt'].astype('float64')

df['Changed_Credit_Limit']=df['Changed_Credit_Limit'].apply(replace_)
df['Changed_Credit_Limit']=df['Changed_Credit_Limit'].replace('',np.nan)
df['Changed_Credit_Limit']=df['Changed_Credit_Limit'].astype('float64')

"""**Replacing the rest unusual/invalid data in other columns, by missing/null values.**
---
"""

df['Occupation']=df['Occupation'].replace('_______',np.nan)
df['Occupation'].value_counts()

"""**Removing invalid age values of customers i.e. age>90 and age<18 and replacing them.**"""

for i in range(len(df['Age'])):
  if df['Age'][i]>90 or df['Age'][i]<18:
    df['Age'][i]=np.nan
  else:
    df['Age'][i]=df['Age'][i]

df['Age'].value_counts()

"""**Here,in the "Num of Loan" column, a customer having loan > 100 seems unrealistic. So we are replacing them as well.**"""

for i in range(len(df['Num_of_Loan'])):
  if df['Num_of_Loan'][i]>=100:
    df['Num_of_Loan'][i]=np.nan
  else:
    df['Num_of_Loan'][i]=df['Num_of_Loan'][i]

df['Num_of_Loan'].value_counts()

"""**Similarly, checking for inconsistency in other columns**"""

df['Delay_from_due_date'].value_counts()

df['Credit_Mix'].value_counts()

df['Credit_Mix']=df['Credit_Mix'].replace('_',np.nan)        #replacing the "_" values

df['Credit_Mix'].value_counts()

df['Amount_invested_monthly'].value_counts()          # here _10000_ is an invalid value

df['Amount_invested_monthly'] = df['Amount_invested_monthly'].str.strip().replace('__10000__', np.nan)
df['Amount_invested_monthly']=df['Amount_invested_monthly'].astype('float')

df['Amount_invested_monthly'].value_counts()

df['Payment_Behaviour'].value_counts()

df['Payment_Behaviour']=df['Payment_Behaviour'].replace('!@9#%8',np.nan)      #replacing !@9#%8  value with null

df['Payment_Behaviour'].value_counts()

df['Monthly_Balance'].value_counts()

df['Monthly_Balance']=df['Monthly_Balance'].replace('__-333333333333333333333333333__' ,np.nan)
df['Monthly_Balance']=df['Monthly_Balance'].apply(replace_)
df['Monthly_Balance']=df['Monthly_Balance'].astype('float64')

df['Monthly_Balance'].value_counts()

df.info()               #cross checking if the numeric features have been converted to float.

df.isna().sum()                  #viewing the count of missing values

"""# **Imputing the missing values**
*   The missing values in the numeric columns have been imputated by their respective means.
*   The missing values in the categorical columns have been imputated by their respective modes.


---
"""

mean_age=df['Age'].mean()
df['Age'].fillna(mean_age,inplace=True)

most_frequent_occupation = df['Occupation'].mode()[0]
df['Occupation'].fillna(most_frequent_occupation, inplace=True)

mval=df['Monthly_Inhand_Salary'].mean()
df['Monthly_Inhand_Salary'].fillna(mval,inplace=True)

mean_numofloan=df['Num_of_Loan'].mean()
df['Num_of_Loan'].fillna(mean_numofloan,inplace=True)

mean_num_of_delayed_payment=df['Num_of_Delayed_Payment'].mean()
df['Num_of_Delayed_Payment'].fillna(mean_num_of_delayed_payment,inplace=True)

mean_changed_credit=df['Changed_Credit_Limit'].mean()
df['Changed_Credit_Limit'].fillna(mean_changed_credit,inplace=True)

mval1=df['Num_Credit_Inquiries'].mean()
df['Num_Credit_Inquiries'].fillna(mval1,inplace=True)

most_frequent_creditmix = df['Credit_Mix'].mode()[0]
df['Credit_Mix'].fillna(most_frequent_creditmix, inplace=True)

most_frequent_credithistory = df['Credit_History_Age'].mode()[0]
df['Credit_History_Age'].fillna(most_frequent_credithistory, inplace=True)

mean_amount_invested=df['Amount_invested_monthly'].mean()
df['Amount_invested_monthly'].fillna(mean_amount_invested,inplace=True)

most_payment_behaviour = df['Payment_Behaviour'].mode()[0]
df['Payment_Behaviour'].fillna(most_payment_behaviour, inplace=True)

mean_monthly_balance=df['Monthly_Balance'].mean()
df['Monthly_Balance'].fillna(mean_monthly_balance,inplace=True)

"""**Transforming the format of data in "Credit history Age" column for simplification.**"""

df['Credit_History_Age'].value_counts()

years  = []
months = []
for value in df["Credit_History_Age"]:
    if value is np.nan:
        years.append(np.nan)
        months.append(np.nan)
    else:
        new_str = value.lower().split()
        years_ = int(new_str[0])                  #storing the years value in the years_ variable
        months_ = int(new_str[new_str.index('and')+1])                    #same for months
        years.append(years_)
        months.append(months_)
df['Credit_Age_years'] = pd.Series(years)
df['Credit_Age_months'] = pd.Series(months)
df.drop('Credit_History_Age',axis=1,inplace=True)

df['Credit_History_Age_years'] = df['Credit_Age_years'] + df['Credit_Age_months'] / 12
df.drop(['Credit_Age_months','Credit_Age_years'],axis=1,inplace=True)

df['Credit_History_Age_years']

df.isnull().sum()     #checking for missing values

plt.figure(figsize=(12,8))
sns.heatmap(df.isnull())

"""**Since, "Name" column cannot be imputed and it doesn't help in classifying the credit score, we dropped it.**"""

df=df.drop(['Name'],axis=1)

df.isnull().sum()

"""# **EDA : Exploratory Data Analysis**
Exploring the data with the help of some visualizations.


---


"""

sns.scatterplot(data=df, x='Annual_Income', y='Credit_Score')
plt.xlabel('Annual Income')
plt.ylabel('Credit Score')
plt.title('Relationship between Annual Income and Credit Score')

# Display the plot
plt.grid(True)
plt.show()

"""This graph shows that there is a weak linear relationship between the Annual Income and the target variable."""

sns.countplot(x=df['Payment_of_Min_Amount'],palette="inferno")

# Add labels and title
plt.xlabel('Payment of minimum Amount')
plt.ylabel('Count')
plt.title('Count of Observations by Payment of minimum Amount')

# Display the plot
plt.show()

"""This graph shows that most of the customers usually pay just the minimum amount required for the credit card."""

plt.figure(figsize=(10, 6))
plt.hist(df['Credit_Score'], bins=20, color='blue', alpha=0.7)
plt.title('Credit score distribution')
plt.xlabel('Credit score')
plt.ylabel('Number of customers')
plt.show()

"""**Through this graph,we observed that our target variable (Credit Score) is unbalanced.**

---

# **Checking and Dealing with Outliers:**

*   After missing values, we are trying to find any noise present in the data. i.e, outliers.

*   We tried to identify the outliers in each feature, by calculating their Inter Quantile Range (IQR) and visualized them using box plots.


---
"""

sns.boxplot(x=df['Age'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Age'} by {'Credit_Score'}")
plt.xlabel('Age')
plt.ylabel('Credit_Score')
plt.show()

sns.boxplot(x=df['Credit_Utilization_Ratio'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Credit_Utilization_Ratio'} by {'Credit_Score'}")
plt.xlabel('Credit_Utilization_Ratio')
plt.ylabel('Credit_Score')
plt.show()

"""Calculating the IQR, lower bound and upper bound, for each category of target variable one by one."""

df_cr1 = df[['Credit_Utilization_Ratio','Credit_Score']].loc[df.Credit_Score=='Good']
Q1_2 = df_cr1['Credit_Utilization_Ratio'].quantile(0.25)
Q3_2 = df_cr1['Credit_Utilization_Ratio'].quantile(0.75)
IQR2 = Q3_2-Q1_2
lower_cr1=Q1_2-1.5*IQR2
upper_cr1=Q3_2+1.5*IQR2

print(lower_cr1,upper_cr1)

df_cr2 = df[['Credit_Utilization_Ratio','Credit_Score']].loc[df.Credit_Score=='Standard']
Q1_2 = df_cr2['Credit_Utilization_Ratio'].quantile(0.25)
Q3_2 = df_cr2['Credit_Utilization_Ratio'].quantile(0.75)
IQR2 = Q3_2-Q1_2
lower_cr2=Q1_2-1.5*IQR2
upper_cr2=Q3_2+1.5*IQR2

print(lower_cr2,upper_cr2)

df_cr3 = df[['Credit_Utilization_Ratio','Credit_Score']].loc[df.Credit_Score=='Poor']
Q1_2 = df_cr3['Credit_Utilization_Ratio'].quantile(0.25)
Q3_2 = df_cr3['Credit_Utilization_Ratio'].quantile(0.75)
IQR2 = Q3_2-Q1_2
lower_cr3=Q1_2-1.5*IQR2
upper_cr3=Q3_2+1.5*IQR2

print(lower_cr3,upper_cr3)

#code for removing outliers
for i,data in df_cr1.iterrows():
  if data['Credit_Utilization_Ratio'] < lower_cr1:
      df.at[i,'Credit_Utilization_Ratio'] = lower_cr1
  elif data['Credit_Utilization_Ratio'] > upper_cr1:
      df.at[i,'Credit_Utilization_Ratio'] = upper_cr1

for i,data in df_cr2.iterrows():
  if data['Credit_Utilization_Ratio'] < lower_cr2:
      df.at[i,'Credit_Utilization_Ratio'] = lower_cr2
  elif data['Credit_Utilization_Ratio'] > upper_cr2:
      df.at[i,'Credit_Utilization_Ratio'] = upper_cr2

for i,data in df_cr3.iterrows():
  if data['Credit_Utilization_Ratio'] < lower_cr3:
      df.at[i,'Credit_Utilization_Ratio'] = lower_cr3
  elif data['Credit_Utilization_Ratio'] > upper_cr3:
      df.at[i,'Credit_Utilization_Ratio'] = upper_cr3

#checking if outliers got removed
sns.boxplot(x=df['Credit_Utilization_Ratio'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Credit_Utilization_Ratio'} by {'Credit_Score'}")
plt.xlabel('Credit_Utilization_Ratio')
plt.ylabel('Credit_Score')
plt.show()

"""Doing the same procedure, for other columns as well."""

#for "Changed_credit_limit" column
sns.boxplot(x=df['Changed_Credit_Limit'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Changed_Credit_Limit'} by {'Credit_Score'}")
plt.xlabel('Changed_Credit_Limit')
plt.ylabel('Credit_Score')
plt.show()

df_2 = df[['Changed_Credit_Limit','Credit_Score']].loc[df.Credit_Score=='Good']
Q1_2 = df_2['Changed_Credit_Limit'].quantile(0.25)
Q3_2 = df_2['Changed_Credit_Limit'].quantile(0.75)
IQR2 = Q3_2-Q1_2
lower2=Q1_2-1.5*IQR2
upper2=Q3_2+1.5*IQR2

print(lower2,upper2)

df_3 = df[['Changed_Credit_Limit','Credit_Score']].loc[df.Credit_Score=='Standard']
Q1_3 = df_3['Changed_Credit_Limit'].quantile(0.25)
Q3_3 = df_3['Changed_Credit_Limit'].quantile(0.75)
IQR2 = Q3_3-Q1_3
lower3=Q1_3-1.5*IQR2
upper3=Q3_3+1.5*IQR2

print(lower3,upper3)

df_4 = df[['Changed_Credit_Limit','Credit_Score']].loc[df.Credit_Score=='Poor']
Q1_4 = df_4['Changed_Credit_Limit'].quantile(0.25)
Q3_4 = df_4['Changed_Credit_Limit'].quantile(0.75)
IQR2 = Q3_4-Q1_4
lower4=Q1_4-1.5*IQR2
upper4=Q3_4+1.5*IQR2

print(lower4,upper4)

for i,data in df_2.iterrows():
  if data['Changed_Credit_Limit'] < lower2:
      df.at[i,'Changed_Credit_Limit'] = lower2
  elif data['Changed_Credit_Limit'] > upper2:
      df.at[i,'Changed_Credit_Limit'] = upper2

for i,data in df_3.iterrows():
  if data['Changed_Credit_Limit'] < lower3:
      df.at[i,'Changed_Credit_Limit'] = lower3
  elif data['Changed_Credit_Limit'] > upper3:
      df.at[i,'Changed_Credit_Limit'] = upper3

for i,data in df_4.iterrows():
  if data['Changed_Credit_Limit'] < lower4:
      df.at[i,'Changed_Credit_Limit'] = lower4
  elif data['Changed_Credit_Limit'] > upper4:
      df.at[i,'Changed_Credit_Limit'] = upper4

sns.boxplot(x=df['Changed_Credit_Limit'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Changed_Credit_Limit'} by {'Credit_Score'}")
plt.xlabel('Changed_Credit_Limit')
plt.ylabel('Credit_Score')
plt.show()

#for "Outstanding_debt" column
sns.boxplot(x=df['Outstanding_Debt'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Outstanding_Debt'} by {'Credit_Score'}")
plt.xlabel('Outstanding_Debt')
plt.ylabel('Credit_Score')
plt.show()

df_od1=df[['Outstanding_Debt','Credit_Score']].loc[df.Credit_Score=='Good']
Q1_od1 = df_od1['Outstanding_Debt'].quantile(0.25)
Q3_od1 = df_od1['Outstanding_Debt'].quantile(0.75)
IQR2 = Q3_od1-Q1_od1
lower_od1=Q1_od1-1.5*IQR2
upper_od1=Q3_od1+1.5*IQR2

print(lower_od1,upper_od1)

df_od2=df[['Outstanding_Debt','Credit_Score']].loc[df.Credit_Score=='Standard']
Q1_od2 = df_od2['Outstanding_Debt'].quantile(0.25)
Q3_od2 = df_od2['Outstanding_Debt'].quantile(0.75)
IQR3 = Q3_od2-Q1_od2
lower_od2=Q1_od2-1.5*IQR3
upper_od2=Q3_od2+1.5*IQR3

print(lower_od2,upper_od2)

df_od3=df[['Outstanding_Debt','Credit_Score']].loc[df.Credit_Score=='Poor']
Q1_od3 = df_od3['Outstanding_Debt'].quantile(0.25)
Q3_od3 = df_od3['Outstanding_Debt'].quantile(0.75)
IQR4 = Q3_od3-Q1_od3
lower_od3=Q1_od3-1.5*IQR4
upper_od3=Q3_od3+1.5*IQR4

print(lower_od3,upper_od3)

for i,data in df_od1.iterrows():
  if data['Outstanding_Debt'] < lower_od1:
      df.at[i,'Outstanding_Debt'] = lower_od1
  elif data['Outstanding_Debt'] > upper_od1:
      df.at[i,'Outstanding_Debt'] = upper_od1

for i,data in df_od2.iterrows():
  if data['Outstanding_Debt'] < lower_od2:
      df.at[i,'Outstanding_Debt'] = lower_od2
  elif data['Outstanding_Debt'] > upper_od2:
      df.at[i,'Outstanding_Debt'] = upper_od2


for i,data in df_od3.iterrows():
  if data['Outstanding_Debt'] < lower_od3:
      df.at[i,'Outstanding_Debt'] = lower_od3
  elif data['Outstanding_Debt'] > upper_od3:
      df.at[i,'Outstanding_Debt'] = upper_od3

sns.boxplot(x=df['Outstanding_Debt'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Outstanding_Debt'} by {'Credit_Score'}")
plt.xlabel('Outstanding_Debt')
plt.ylabel('Credit_Score')
plt.show()

#for "Amount invested monthly" column
sns.boxplot(x=df['Amount_invested_monthly'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Amount_invested_monthly'} by {'Credit_Score'}")
plt.xlabel('Amount_invested_monthly')
plt.ylabel('Credit_Score')
plt.show()

df_a1=df[['Amount_invested_monthly','Credit_Score']].loc[df.Credit_Score=='Good']
Q1_a1 = df_a1['Amount_invested_monthly'].quantile(0.25)
Q3_a1 = df_a1['Amount_invested_monthly'].quantile(0.75)
IQR2 = Q3_a1-Q1_a1
lower_a1=Q1_a1-1.5*IQR2
upper_a1=Q3_a1+1.5*IQR2
print(lower_a1,upper_a1)

df_a2=df[['Amount_invested_monthly','Credit_Score']].loc[df.Credit_Score=='Standard']
Q1_a2 = df_a2['Amount_invested_monthly'].quantile(0.25)
Q3_a2 = df_a2['Amount_invested_monthly'].quantile(0.75)
IQR2 = Q3_a2-Q1_a2
lower_a2=Q1_a2-1.5*IQR2
upper_a2=Q3_a2+1.5*IQR2
print(lower_a2,upper_a2)

df_a3=df[['Amount_invested_monthly','Credit_Score']].loc[df.Credit_Score=='Poor']
Q1_a3 = df_a3['Amount_invested_monthly'].quantile(0.25)
Q3_a3 = df_a3['Amount_invested_monthly'].quantile(0.75)
IQR2 = Q3_a3-Q1_a3
lower_a3=Q1_a3-1.5*IQR2
upper_a3=Q3_a3+1.5*IQR2
print(lower_a3,upper_a3)

for i,data in df_a1.iterrows():
  if data['Amount_invested_monthly'] < lower_a1:
      df.at[i,'Amount_invested_monthly'] = lower_a1
  elif data['Amount_invested_monthly'] > upper_a1:
      df.at[i,'Amount_invested_monthly'] = upper_a1

for i,data in df_a2.iterrows():
  if data['Amount_invested_monthly'] < lower_a2:
      df.at[i,'Amount_invested_monthly'] = lower_a2
  elif data['Amount_invested_monthly'] > upper_a2:
      df.at[i,'Amount_invested_monthly'] = upper_a2

for i,data in df_a3.iterrows():
  if data['Amount_invested_monthly'] < lower_a3:
      df.at[i,'Amount_invested_monthly'] = lower_a3
  elif data['Amount_invested_monthly'] > upper_a3:
      df.at[i,'Amount_invested_monthly'] = upper_a3

sns.boxplot(x=df['Amount_invested_monthly'], y=df['Credit_Score'])
plt.title(f"Box Plot of {'Amount_invested_monthly'} by {'Credit_Score'}")
plt.xlabel('Amount_invested_monthly')
plt.ylabel('Credit_Score')
plt.show()

"""

---

**Since, ID, Customer ID and Social Security number are typically used for identification purposes and are not directly related to credit worthiness or credit score. We dropped them.**"""

df=df.drop(['ID','Customer_ID','SSN'],axis=1)

"""# **Standardization: Scaling the features**
* As our dependent features have drastic difference in their values or scales between them (such as Age and Annual Income), we are Scaling our features.
* We have divided our features into 2 lists: **numeric and categorical**.
* We used the numeric feature list for scaling every numeric column, and since the range of our feature is unknown, we have used Standard Scalar.

---


"""

numeric_features = []
categorical_features = []
for column in df.columns:
    if pd.api.types.is_numeric_dtype(df[column]):
        numeric_features.append(column)
    else:
        categorical_features.append(column)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
data_scaled=scaler.fit_transform(df[numeric_features])
df_scaled=pd.DataFrame(data_scaled,columns=df[numeric_features].columns)
df_scaled

"""---


# **Encoding the features**
Now, we are encoding our categorical variables to convert them to numeric, as most models don't accept categorical values as their input.

**For the below columns, because they didn't have any order, we have used One-Hot encoding as our encoder.**
"""

df_Occupation = pd.get_dummies(df['Occupation'], prefix='Occupation')
df_Occupation

df_Credit_Mix = pd.get_dummies(df['Credit_Mix'], prefix='Credit_Mix')
df_Credit_Mix

pay_behav_encoded = pd.get_dummies(df['Payment_Behaviour'])
pay_behav_encoded

pay_min_amount_encoded = pd.get_dummies(df['Payment_of_Min_Amount'],prefix="Pay_min_amm")
pay_min_amount_encoded

"""**Here, we are encoding the months, by replacing every month with a number, in their respective order.**"""

Month_encode = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12}
df['Month']= df['Month'].map(Month_encode)
df['Month']

Month_encoded=df['Month']
Credit_Score_encoded=df['Credit_Score_encoded']

"""**Here, we have encoded our target variable, Credit Score, with Label Encoder (as it has a specific order - Poor, Standard, Good)**"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['Credit_Score_encoded']= label_encoder.fit_transform(df['Credit_Score'])
df['Credit_Score_encoded']

#dropping the original columns after encoding them
df.drop(['Occupation','Credit_Mix','Payment_Behaviour','Payment_of_Min_Amount','Credit_Score'],axis=1,inplace=True)
df

df_1=df[['Month','Credit_Score_encoded']]
df_1

df_new=pd.concat([df_scaled,Month,df_Occupation,pay_behav_encoded,pay_min_amount_encoded,df_Credit_Mix],axis=1)
df_new

"""**Concatenating/Adding all the encoded columns with the original df and storing it in a new dataframe.**"""

df_new=pd.concat([df_scaled,df_1,df_Occupation,pay_behav_encoded,pay_min_amount_encoded,df_Credit_Mix],axis=1)
df_new

"""# **Splitting the dataset**
* Now, we are splitting the dataset into **train-test** splits, so that we can train our model and test it's performance later.
* We have set 80% of our data for training and 20% for the testing data.


---




"""

from sklearn.model_selection import train_test_split
x=df_new.drop(['Credit_Score_encoded'],axis=1)
y=df_new['Credit_Score_encoded']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

#to print the number of records for training and testing data
train_data = pd.concat([X_train, y_train], axis=1)
test_data = pd.concat([X_test, y_test], axis=1)
print("Number of records in training data:", len(train_data))
print("Number of records in testing data:", len(test_data))

"""# **Sampling**

**Since, we observed earlier that our target variable is unbalanced, so as to not create any biases in our prediction, we used sampling techniques like SMOTE to balance our target variable.**


---




"""

labels = df_new["Credit_Score_encoded"].value_counts().index
sizes = df_new["Credit_Score_encoded"].value_counts()

plt.figure(figsize = (10,10))
plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Credit_Score Percentage',color = 'black',fontsize = 30)
plt.legend(df_new["Credit_Score_encoded"].value_counts())
plt.show()

from imblearn.over_sampling import SMOTE              #sampling code
x=df_new.drop(['Credit_Score_encoded'],axis=1)
y=df_new['Credit_Score_encoded']

smote=SMOTE()
x_balanced,y_balanced=smote.fit_resample(X_train,y_train)

data_smote= pd.DataFrame(x_balanced, columns=x.columns)
data_smote['Credit_Score_encoded'] = y_balanced

data_smote['Credit_Score_encoded']                #data after sampling

labels = data_smote["Credit_Score_encoded"].value_counts().index
sizes = data_smote["Credit_Score_encoded"].value_counts()

plt.figure(figsize = (10,10))
plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Credit_Score Percentage',color = 'black',fontsize = 30)
plt.legend(data_smote["Credit_Score_encoded"].value_counts())
plt.show()

"""# **Training the model**
As our model is based upon **classification** of credit scores, we have used the following 4 algorithms to train our model -
* Random Forest
* XGBoost
* SVM
* Gradient Boosting

---

**1. Training our model based on Random Forest Algorithm.**
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
RF = RandomForestClassifier(n_estimators=100, random_state=42)
RF.fit(x_balanced,y_balanced)

RF.score(x_balanced,y_balanced)               #prediction score on training data

RF.score(X_test,y_test)                        #prediction score on testing data

"""**The accuracy we achieved in Random Forest Algorithm was 82%**"""

Y_pred=RF.predict(X_test)
Y_pred

#model overall report
report = classification_report(y_test, Y_pred)
print(report)

"""**Confusion Matrix for Random Forest Algorithm**"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, pred, labels=RF.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=RF.classes_)
disp.plot()
plt.show()

"""**2. Training our model using XGBoost Algorithm.**"""

import xgboost as xgb

xgb = xgb.XGBClassifier()
xgb.fit(x_balanced,y_balanced)

xgb.score(x_balanced,y_balanced)             #training score

xgb.score(X_test,y_test)                      #testing score

"""**The accuracy we achieved in XGBoost Algorithm was 79%.**"""

Y_pred_xgb=xgb.predict(X_test)
Y_pred_xgb

#XGBoost overall model report
report1 = classification_report(y_test, Y_pred_xgb)
print(report1)

"""**Confusion Matrix for XGboost Algorithm.**"""

cm1 = confusion_matrix(y_test, Y_pred_xgb, labels=xgb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=xgb.classes_)
disp.plot()
plt.show()

"""**3. Training our model using SVM (Support Vector Machine) Algorithm.**"""

from sklearn.svm import SVC
# Create SVC classifier with linear kernel and adjusted C value
svm_classifier = SVC(kernel='linear', C=0.1, random_state=42)
# Fit the classifier to the balanced dataset
svm_classifier.fit(x_balanced, y_balanced)

svm_classifier.score(x_balanced,y_balanced)

svm_classifier.score(X_test,y_test)

"""**The accuracy we achieved in SVM Algorithm was 63%**

---

**4. Training our model using Gradient Boosting Algorithm.**
"""

from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm.fit(x_balanced, y_balanced)

gbm.score(x_balanced,y_balanced)

gbm.score(X_test,y_test)

"""**The accuracy we achieved in Gradient Boosting Algorithm was 77%**"""

Y_pred_gbm=gbm.predict(X_test)
Y_pred_gbm

#overall model report
report2 = classification_report(y_test, Y_pred_gbm)
print(report2)

"""**Confusion Matrix for Gradient Boosting Algorithm.**"""

cm2 = confusion_matrix(y_test, Y_pred_gbm, labels=gbm.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=gbm.classes_)
disp.plot()
plt.show()

"""# **Feature Selection**

---


"""

from sklearn.feature_selection import SelectKBest, chi2
X_log = np.log1p(np.abs(X_train))
k_best = SelectKBest(score_func=chi2, k=23)
X_train_selected = k_best.fit_transform(X_log, y_train)

selected_feature_names = X_train.columns[k_best.get_support()]
print("Selected Feature Names:", selected_feature_names)

X_train_selected = x_balanced[selected_feature_names]
X_test_selected = X_test[selected_feature_names]

RF_f= RandomForestClassifier(n_estimators=100, random_state=42)
RF_f.fit(X_train_selected,y_balanced)

RF_f.score(X_train_selected,y_balanced)

RF_f.score(X_test_selected ,y_test)

"""**After doing feature selection, our accuracy for the Random Forest algorithm dropped, from 82 to 79. Hence, we didn't opt for feature selection.**

---

# **HyperParameter Tuning**
We used Randomized SearchCV to find the optimal values of our paramenters, in order to increase the performance of our model.

---
"""

from sklearn.model_selection import RandomizedSearchCV
param_space = {
    'n_estimators': (50, 100),# Number of trees in the forest
    'max_depth': (2, 10),             # Maximum depth of the trees
    'min_samples_split': (2, 10),      # Minimum number of samples required to split an internal node
    'min_samples_leaf': (1, 4),         # Minimum number of samples required to be at a leaf node
    'max_features': (0.2, 0.6, 1.0), #Number of features to consider at every split
    'max_samples': (0.5,0.75,1.0),
    'bootstrap': (True,False)
    }
random_search = RandomizedSearchCV(estimator=RF, param_distributions=param_space, n_iter=50, cv=5, scoring='accuracy',n_jobs=-1)

random_search.fit(x_balanced, y_balanced)

random_search.best_score_              #Our accuracy decreased after tuning.

"""After tuning, our accuracy dropped to 79%.

---

# **Hence, we consider our first model i.e, the Random Forest Algorithm model to be our best model, with accuracy as 83%.**
"""